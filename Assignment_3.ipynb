{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff0687d2",
   "metadata": {},
   "source": [
    "## 2. Data Collection and Pre-processing\n",
    "\n",
    "### 2.1 The Challenge: Continuous Stream vs. Discrete Phases\n",
    "The raw data collected in the previous experiment consisted of 15 contiguous gameplay sessions recorded in a single video file. This presented a significant noise challenge: the recording contained loading screens, menu navigations, and three distinct boss phases (Potato, Onion, Carrot), each requiring different visual recognition patterns and reflex strategies.\n",
    "\n",
    "To train a specialized agent, I needed to isolate the **Potato Phase**â€”the specific segment where the player must dodge projectiles (dirt clods) and jump over obstacles.\n",
    "I created segmentation script and filtered dataset genreator to solve this problem (see appendix X)\n",
    "\n",
    "\n",
    "\n",
    "## 3. Methodology: Pivoting from Generalization to Micro-Timing\n",
    "\n",
    "### 3.1 Refinement of Scope\n",
    "In Assignment 2, the \"General Cuphead Agent\" achieved high accuracy (96%) but failed functionally because it prioritized the majority class (shooting) and struggled to \"see\" small, high-velocity threats in the Autoencoder's latent space.\n",
    "\n",
    "For this final experiment, I pivoted from a broad strategy (playing the whole game) to a focused **micro-timing analysis**. The core research question shifted:\n",
    "> *Does Explicit Perception (Object Detection) outperform Implicit Perception (Latent Representation) in high-speed reaction tasks?*\n",
    "\n",
    "To answer this, I restricted the domain to the **Potato Phase**. This phase is mechanically deterministic but visually demanding, making it the perfect \"lab environment\" to compare how well different vision architectures can track projectile velocity and trigger a jump action.\n",
    "\n",
    "\n",
    "\n",
    "### 3.2 Model Selection and Architecture Comparison\n",
    "\n",
    "I designed two distinct pipelines to compete against each other:\n",
    "\n",
    "#### **Pipeline A: The Baseline (Implicit Perception)**\n",
    "*   **Vision:** Convolutional Autoencoder (CAE).\n",
    "*   **Decision:** Gated Recurrent Unit (GRU).\n",
    "*   **Rationale:** This represents the \"End-to-End\" Deep Learning approach. The model is not told what a \"bullet\" is; it must learn to represent valid game states in a latent vector $z$. The GRU is necessary here to infer velocity, as a single static frame's latent vector does not contain motion information. The RNN must \"remember\" the previous latent states to understand if an object is moving toward the player.\n",
    "\n",
    "\n",
    "# make CAE GRU for potato phase\n",
    "\n",
    "#### **Pipeline B: The Novel Approach (Explicit Perception via YOLO)**\n",
    "*   **Vision:** **YOLOv8 (You Only Look Once)**.\n",
    "*   **Decision:** Multi-Layer Perceptron (MLP).\n",
    "*   **Rationale:** This is the novel technique introduced for this assignment. Instead of relying on the neural network to figure out what pixels matter, we explicitly teach it.\n",
    "    1.  **Object Detection:** YOLO detects discrete entities: `Cuphead`, `Potato`, and `Projectile`.\n",
    "    2.  **State Extraction:** We convert bounding boxes into a structured state vector containing **Physics Features** (Distance to projectile, Relative Velocity).\n",
    "    3.  **Logic:** Because we explicitly calculate velocity from the YOLO detections across frames ($v = \\frac{\\Delta x}{\\Delta t}$), we do **not** need a complex Recurrent Neural Network (GRU). A simple Feed-Forward Network (MLP) should be sufficient to map the state vector $[x, y, v_x, v_y]$ to the action `JUMP`.\n",
    "\n",
    "### 3.3 Hypothesis\n",
    "I hypothesize that **Pipeline B (YOLO)** will significantly outperform Pipeline A in **Recall** (catching rare Jump events). The Autoencoder in Pipeline A tends to blur small, fast-moving projectiles as \"noise\" during compression. YOLO, trained specifically on those objects, will force the decision model to acknowledge their existence, creating a distinct \"Decision Boundary\" that mimics human reaction timing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1847b9",
   "metadata": {},
   "source": [
    "# appendix\n",
    "\n",
    "\n",
    " ### 2.2 Custom Segmentation Tooling\n",
    "Standard video editing tools were insufficient because I needed to preserve the exact millisecond alignment between video frames and the asynchronous keystroke logs (UTC timestamps).\n",
    "\n",
    "To solve this, I developed a custom Python utility, `mark_potato_segments.py`, which utilizes OpenCV and the original `_frames.jsonl` log file.\n",
    "*   **Frame-Accurate Navigation:** The tool allowed me to step through the raw training video frame-by-frame.\n",
    "*   **Metadata Tagging:** I manually identified the start (the moment the \"Fight!\" banner disappears) and end (the moment the Potato retreats) of every Potato phase across all 15 sessions.\n",
    "*   **UTC Extraction:** Instead of cutting the video files (which risks re-encoding artifacts and frame drift), the tool generated a lightweight JSON metadata file (`potato_phase_segments.json`).\n",
    "\n",
    "**Segment Metadata Structure:**\n",
    "```json\n",
    "{\n",
    "    \"id\": 1,\n",
    "    \"start_frame\": 450,\n",
    "    \"start_utc\": 1763997299.05,\n",
    "    \"end_frame\": 1200,\n",
    "    \"end_utc\": 1763997324.10\n",
    "}\n",
    "```\n",
    "\n",
    "### 2.3 Dataset Filtering\n",
    "Using this metadata, I created a filtered dataset generator. During training, the data loader reads the JSON segments and only yields frames and keystrokes that fall strictly within the active Potato Phase combat windows. This reduced the dataset size but drastically increased data quality, ensuring the model only learns from relevant combat frames."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
