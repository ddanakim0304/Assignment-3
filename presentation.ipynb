{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31ad7978",
   "metadata": {},
   "source": [
    "In Assignment 2, the agent was trained using **Behavioral Cloning** (Supervised Learning). While effective for feature extraction, the agent suffered from distribution shift. \n",
    "\n",
    "For the final assignment, I implemented **Proximal Policy Optimization (PPO)**, a state-of-the-art policy gradient method. This required building a custom `Gym` environment that wraps the *Cuphead* application, using Computer Vision to calculate rewards (HP loss) in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e9ce7e",
   "metadata": {},
   "source": [
    "### 1. Dependencies\n",
    "We utilize `stable-baselines3` for the PPO implementation and `mss` for high-speed screen capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56ce4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary RL libraries\n",
    "!pip install stable-baselines3 gym mss pydirectinput opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036b588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mss\n",
    "import pydirectinput\n",
    "import time\n",
    "from stable_baselines3 import PPO\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Global Constants\n",
    "LATENT_DIM = 512\n",
    "IMG_WIDTH = 128\n",
    "IMG_HEIGHT = 72"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28615b7",
   "metadata": {},
   "source": [
    "### 2. The Reward Function (Computer Vision)\n",
    "Since *Cuphead* provides no API, we must calculate the Reward function visually. We monitor the bottom-left corner of the screen (the HP cards). If the average red intensity of that region drops below a threshold, we know the player has taken damage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5c42b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_visual_reward(screen_frame):\n",
    "    \"\"\"\n",
    "    Analyzes specific screen region to determine game state.\n",
    "    Returns:\n",
    "        reward (float): +0.1 for survival, -10.0 for taking damage.\n",
    "        done (bool): True if HP is 0 or Level Complete.\n",
    "    \"\"\"\n",
    "    hp_region = screen_frame[650:700, 20:100]\n",
    "    \n",
    "    # Calculate Red Channel Intensity\n",
    "    # If the card flips (damage taken), the red/pink turns to grey\n",
    "    avg_red = np.mean(hp_region[:, :, 2]) # OpenCV uses BGR, index 2 is Red\n",
    "    \n",
    "    # Threshold determined experimentally\n",
    "    if avg_red < 50: \n",
    "        return -10.0, True # Penalty and End Episode (simplified for training)\n",
    "    \n",
    "    # Small positive reward for every frame survived\n",
    "    return 0.1, False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355c7d87",
   "metadata": {},
   "source": [
    "### 3. The Custom Gym Environment\n",
    "This class bridges the gap between the RL agent and the game application. It adheres to the standard OpenAI Gym API (`step`, `reset`, `render`).\n",
    "\n",
    "**Key Architecture Decision:** We reuse the **Autoencoder** trained in Assignment 2. We do *not* retrain the vision layer. The environment captures a frame, passes it through the frozen Autoencoder, and yields the 512-dimensional Latent Vector as the `observation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04591eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CupheadEnv(gym.Env):\n",
    "    def __init__(self, encoder_path):\n",
    "        super(CupheadEnv, self).__init__()\n",
    "        \n",
    "        # 1. Load Pre-trained Vision Model (Frozen)\n",
    "        print(\"Loading Autoencoder...\")\n",
    "        self.encoder = load_model(encoder_path)\n",
    "        \n",
    "        # 2. Define Action Space (Discrete Buttons)\n",
    "        # 0: No Op, 1: Left, 2: Right, 3: Jump, 4: Shoot, 5: Dash\n",
    "        self.action_space = spaces.Discrete(6)\n",
    "        \n",
    "        # 3. Define Observation Space (Latent Vector)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(LATENT_DIM,), dtype=np.float32)\n",
    "        \n",
    "        # 4. Screen Capture Setup\n",
    "        self.sct = mss.mss()\n",
    "        self.monitor = {\"top\": 100, \"left\": 0, \"width\": 1280, \"height\": 720}\n",
    "\n",
    "    def step(self, action):\n",
    "        # A. Execute Action in Game\n",
    "        self._perform_input(action)\n",
    "        \n",
    "        # Wait for frame update (simulating reaction time)\n",
    "        time.sleep(0.05)\n",
    "        \n",
    "        # B. Capture Environment State\n",
    "        screen = np.array(self.sct.grab(self.monitor))\n",
    "        \n",
    "        # C. Process Vision (Resize -> Normalize -> Autoencode)\n",
    "        screen_proc = cv2.resize(screen, (IMG_WIDTH, IMG_HEIGHT))\n",
    "        screen_proc = cv2.cvtColor(screen_proc, cv2.COLOR_BGRA2GRAY)\n",
    "        screen_proc = screen_proc.astype('float32') / 255.0\n",
    "        \n",
    "        # Get Latent Vector (Observation)\n",
    "        # We expand dims because model expects batch (1, 72, 128, 1)\n",
    "        observation = self.encoder.predict(np.expand_dims(screen_proc, axis=[0, -1]), verbose=0)[0]\n",
    "        \n",
    "        # D. Calculate Reward\n",
    "        reward, done = calculate_visual_reward(screen)\n",
    "        \n",
    "        return observation, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        # Sequence to restart the level in Cuphead\n",
    "        pydirectinput.press('r') \n",
    "        time.sleep(2.0)\n",
    "        \n",
    "        # Return initial observation\n",
    "        screen = np.array(self.sct.grab(self.monitor))\n",
    "        screen_proc = cv2.resize(screen, (IMG_WIDTH, IMG_HEIGHT))\n",
    "        screen_proc = cv2.cvtColor(screen_proc, cv2.COLOR_BGRA2GRAY).astype('float32') / 255.0\n",
    "        return self.encoder.predict(np.expand_dims(screen_proc, axis=[0, -1]), verbose=0)[0]\n",
    "\n",
    "    def _perform_input(self, action):\n",
    "        # Map RL outputs to Keystrokes\n",
    "        if action == 1: pydirectinput.press('left')\n",
    "        elif action == 2: pydirectinput.press('right')\n",
    "        elif action == 3: pydirectinput.press('z') # Jump\n",
    "        elif action == 4: pydirectinput.press('x') # Shoot\n",
    "        elif action == 5: pydirectinput.press('c') # Dash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cf04f4",
   "metadata": {},
   "source": [
    "### 4. PPO Agent Initialization and Training\n",
    "\n",
    "We use the **Proximal Policy Optimization** algorithm. PPO is an Actor-Critic method that optimizes a surrogate objective function:\n",
    "\n",
    "$$ L^{CLIP}(\\theta) = \\hat{E}_t [\\min(r_t(\\theta)\\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t)] $$\n",
    "\n",
    "The clipping term $(1-\\epsilon, 1+\\epsilon)$ ensures that the policy does not change too drastically in a single update, providing the stability needed for a difficult game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4435439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Environment\n",
    "env = CupheadEnv(encoder_path=\"../models/2_encoder_512.keras\")\n",
    "\n",
    "# Initialize PPO Agent\n",
    "# 'MlpPolicy': We use a Multi-Layer Perceptron because our input is \n",
    "# the 1D Latent Vector, not the raw image pixels.\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, learning_rate=0.0003)\n",
    "\n",
    "print(\"Starting RL Training Loop...\")\n",
    "# We train for 10,000 timesteps for the demonstration\n",
    "# In a full deployment, this would be >100,000\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53651c1",
   "metadata": {},
   "source": [
    "### 5. Saving the Policy\n",
    "Once trained, the PPO policy is saved. This small file contains the \"brain\" that can be loaded to play the game autonomously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269cea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"cuphead_ppo_policy_v1\")\n",
    "print(\"Model Saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
